hydra:
  run:
    dir: ./output

general:
  batch_size: 8
  n_fft: 159 # -> output 80 filterbank for spectrogram
  lr: 0.001
  log_idx: 100
  epochs: 500
  
text:
  all_types:
    - char
    - bpe
  hyper:
    char:
      lang: vi
    bpe:
      vocab_size: 1000
      pct_bpe: 1
  selected: char

dataset:
  all_types:
    - vivos
    - compose
  hyper:
    vivos:
      root: data/vivos-dataset/vivos
      n_fft: ${general.n_fft}
    compose:
      vivos_root: data/vivos
      vlsp_root: data/vlsp2020_train_set_02
      n_fft: ${general.n_fft}
  selected: compose

datamodule:
  persistent_workers: False
  num_workers: 0

model:
  encoder:
    all_types:
      - vgg
      - lstm
      - transformer
      - conformer
    structure:
      - conformer
    hyper:
      general:
        input_dim: 80 # n_fft // 2 + 1
      conformer:
        encoder_dim: 8 # 144
        num_encoder_layers: 1 # 8
        num_attention_heads: 1
        freq_masks: 2
        time_masks: 10
        freq_width: 27
        time_width: 0.05
      vgg:
        init_dim: 32
        hide_dim: 64
      lstm:
        hidden_size: 32
        num_layers: 1
        bias: True
        batch_first: True 
        dropout: 0.1
        bidirectional: True
      transformer: 
        d_model:  ${model.encoder.hyper.conformer.encoder_dim}
        nhead: 1
        num_layers: 1
        dim_feedforward: 32
        dropout: 0.1
        activation: relu # or gelu
        layer_norm_eps: 1e-05
        batch_first: True
        norm_first: False
  decoder:
    all_types:
      - lstm
      - transformer
      - none
    selected: none
    hyper:
      lstm:
        hidden_size: 32
        num_layers: 1
        use_attention: False
        num_attention_heads: 1
        bias: True
        batch_first: True
        dropout: 0.1
        bidirectional: False
      transformer:
        d_model: ${model.encoder.hyper.transformer.d_model}
        nhead: 1
        num_layers: 1
        dim_feedforward: 32
        dropout: 0.1
        activation: relu
        layer_norm_eps: 1e-05
        batch_first: True
        norm_first: False
  framework:
    all_types:
      - ctc
      - aed
      - rnnt
      - joint_ctc_attention
    selected: ctc
    hyper:
      ctc:
        log_idx: ${general.log_idx}
      aed:
        log_idx: ${general.log_idx}
      rnnt:
        log_idx: ${general.log_idx}
      joint_ctc_attention:
        ctc_weight: 0.4
        log_idx: ${general.log_idx}
  loss:
    ctc:
      blank: 0
      zero_infinity: True
    cross_entropy:
      ignore_index: 0
      reduce: True
      reduction: mean
    rnnt:
      blank: 0
      reduction: mean
  optim:
    adamw:
      lr: ${general.lr}
      betas: [0.9, 0.98]
      eps: 1e-08
      weight_decay: 0.01
  lr_scheduler:
    one_cycle_lr:
      max_lr: ${general.lr}
      epochs: ${general.epochs}
      steps_per_epoch: -1
      verbose: True

trainer:
  tb_logger:
    save_dir: tb_logs
    name: model_logs
  lr_monitor:
    logging_interval: step
  hyper:
    max_epochs: ${general.epochs}
    accelerator: cuda # auto
    accumulate_grad_batches: 4
    devices: -1
    precision: 16
    amp_backend: apex
    amp_level: 03

session:
  train: True
  validate: False
  test: False
  predict:
    is_pred: False
    audio_path: /home/tuanio/workspace/LibriSpeech/test-other/3331/159609/3331-159609-0000.flac

ckpt:
  use_ckpt: False
  ckpt_path: ../input/checkpoint/epoch145-step310688.ckpt